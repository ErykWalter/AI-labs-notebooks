{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deep reinforcement learning\n",
    "\n",
    "Some introductory remarks:\n",
    "\n",
    "1. This is not an assignment, this is a demo to make learning about the deep reinforcement learning easier. Run it. Observe its behaviour. Change it. Run it again. And change it again. And again.\n",
    "2. While developing this demo I found some on-line resources helpful (but neither covers exactly the scope of this demo):\n",
    "    * https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\n",
    "    * https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "3. Some things are written in a suboptimal way to make the code simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /home/eryczek/.local/lib/python3.10/site-packages (from gym) (1.23.2)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Collecting cloudpickle>=1.2.0\n",
      "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827635 sha256=05e2480261071671fb5c511970ef1fdc0cfeb4694850b26021615be8fb2707e1\n",
      "  Stored in directory: /home/eryczek/.cache/pip/wheels/ae/5f/67/64914473eb34e9ba89dbc7eefe7e9be8f6673fbc6f0273b29f\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cloudpickle, gym\n",
      "Successfully installed cloudpickle-2.2.0 gym-0.26.2 gym-notices-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing an environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning requires an agent and an environment for the agent. We will use one of the environments provided by the *OpenAI Gym* called *CartPole*.\n",
    "The following description is copied from https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
    "\n",
    "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "**Note:** There are multiple environment available. Feel free to experiment with them, but remember that this may require substantial changes in the hyperparameters and/or in the agent's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each environment in the OpenAI Gym has some action space (here 2 different actions are possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an observation space (here consisting of 4 continous variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the environment in action: the code below performs random actions, which are visualised (`env.render()`) for your enjoyment. Observe that a only a handful of actions can be executed this way before the episode ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished after 8 steps\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "for t in range(100):\n",
    "    env.render()    \n",
    "    action = env.action_space.sample()\n",
    "    newstate, reward, done, truncated, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Finished after\", t, \"steps\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing an agent\n",
    "\n",
    "We create two identical neural networks, one called `target` and the other called `policy`. Their purpose is to approximate the function $Q(s,a)$, but in a somewhat special way: an input of a model is the current state `s` and the outputs are the values of $Q(s, a)$ for each possible action.\n",
    "\n",
    "The explanation why there are two models is a bit complicated: recall that in the Q-learning algorithm the agent updates $Q(s,a)$ to the value of $r+\\gamma \\max_{a'} Q(s', a')$ (where $r$ stands for a reward received after transitioning from $s$ to $s'$ using the action $a$). Now, should we use the same network to approximate both the current value $Q(s,a)$ and the target value $r+\\gamma \\max_{a'} Q(s', a')$ it would mean that changing the parameters of the network changes both the current value $Q(s,a)$ and the target value, which is perhaps rather unfortunate. Instead, we use an off-line copy of the network to compute the target value and perdiocially (and rather rarely) update it by copying the parameters of the on-line network to the off-line network. \n",
    "\n",
    "In the code below the variable `policy` represents the on-line model which is used to compute $Q(s,a)$ and `target` represents the off-line model used to compute $Q(s',a')$ in the target value. Both models have an identical structure defined by the function `create_model` and the call to the function `target.load_state_dict` ensures that both are initialized with exactly the same values of parameters.\n",
    "\n",
    "**Note**: Consider modifying the used model, e.g., remove `BatchNorm1d`, add/remove a layer, make a layer wider/narrower, change nonlinearity. These changes should not require substantial changes in the other parts of the code. You can also try to remove the second model (`target`) alltogether (`target=policy` should do the trick, after all variables in Python store references, not the objects themselves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "n_observations = sum(env.observation_space.shape)\n",
    "\n",
    "def create_model():\n",
    "    return nn.Sequential(        \n",
    "        nn.BatchNorm1d(n_observations),\n",
    "        nn.Sequential(\n",
    "            nn.Linear(n_observations, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU()\n",
    "        ),\n",
    "        nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU()\n",
    "        ),\n",
    "        nn.Linear(32, n_actions)\n",
    "    )\n",
    "\n",
    "target = create_model()\n",
    "policy = create_model()\n",
    "target.load_state_dict(policy.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loss function we use the smooth L1 loss (https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html) and to optimize the parameters (of the `policy` network only!) we use the Adam optimizer.\n",
    "\n",
    "**Note**: Both are some arbitrary choices and you're free to experiment with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn = nn.SmoothL1Loss()\n",
    "opt = optim.Adam(policy.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function `training_step` updates the `policy` network using the given `experience`, which is an iterable of 5-tuples. Each of the tuples represents a piece of experience - a single transition that occured sometime during the interaction with the environment and consists of the following:\n",
    "\n",
    "0. The original state\n",
    "1. The action executed in the original state\n",
    "2. The reward received immediately after executing this action\n",
    "3. The state achieved immediately after exeucting this action\n",
    "4. A boolean value: `True` if the episode ended after this action and `False` otherwise\n",
    "\n",
    "The tuples of experience are separated into five tensors `batch_*` and then first `qtarget` is computed:\n",
    "1. the `target` network is used to estimate $\\max_{a'} Q(s', a')$;\n",
    "2. the estimates are set to 0 for the states that are final and no further actions (thus: no further rewards) are possible; (**Note**: What happens if you remove this line?)\n",
    "3. the final target values are computed according to the formula $r+\\gamma\\max_{a'} Q(s',a')$\n",
    "\n",
    "The current values $Q(s,a)$ are then estimated using the `policy` network: first, the values $Q(s,a)$ are estimated for each state in `batch_states` and each available action and then the values for the executed actions are selected using `torch.gather` (which is basically an elaborate indexing function).\n",
    "\n",
    "Finally, a typical training commences: the gradients are zeroed, the value of the loss function is computed, then its gradients are computed and the optimization step is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(experience, gamma):    \n",
    "    batch_states = torch.as_tensor(np.array([e[0] for e in experience]), dtype=torch.float)\n",
    "    batch_actions = torch.as_tensor(np.array([e[1] for e in experience]), dtype=torch.long)\n",
    "    batch_rewards = torch.as_tensor(np.array([e[2] for e in experience]), dtype=torch.float)\n",
    "    batch_newstates = torch.as_tensor(np.array([e[3] for e in experience]), dtype=torch.float)\n",
    "    batch_final = torch.as_tensor(np.array([e[4] for e in experience]), dtype=torch.bool)\n",
    "    \n",
    "    qtarget = torch.max(target(batch_newstates), dim=1)[0]                \n",
    "    qtarget[batch_final] = 0\n",
    "    qtarget = batch_rewards+gamma*qtarget\n",
    "    \n",
    "    policy.train()    \n",
    "    qpolicy = policy(batch_states)        \n",
    "    qpolicy = torch.gather(qpolicy, dim=1, index=batch_actions.reshape((-1, 1))).reshape(-1)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss = lossfn(qtarget, qpolicy)\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following auxiliary function draws a chart containing the rewards in each episode and a moving average. The moving average is useful to observe the trend rather than the exact values of the rewards in each episodes, which can vary greately from one episode to the next. The function uses some features of IPython/Jupyter Notebook in order to update the chart during the training instead of displaying it after the training finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chart(total_rewards, window=100):\n",
    "    try:\n",
    "        fig = plt.gcf()\n",
    "        fig.clear()\n",
    "        plt.plot(total_rewards)            \n",
    "        if len(total_rewards) > window:\n",
    "            ma = np.concatenate((np.zeros(window), np.convolve(total_rewards, np.ones(window)/window, mode='valid')))\n",
    "            plt.plot(ma)\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training, we will use the $\\epsilon$ method: with some probability $\\epsilon$ the agent will take a random action, and with the remaining probability $1-\\epsilon$ it will take the best action according to the network `policy` (i.e., the action with the highest value $Q(s,a)$). We will change $\\epsilon$ so that it is high in the beginning of the training (a lot of exploration) and low in the end of the training (preferring exploitation over exploration). The function `p_exploration` defined below returns the value of $\\epsilon$ for the given number of the episode during the training.\n",
    "\n",
    "**Note**: You can freely modify all three hyperparameters of the function (`eps_decay`, `max_eps` and `min_eps`) and the formula of the function itself. How do these change influence the speed and effects of learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_exploration(i_episode):\n",
    "    eps_decay = 500\n",
    "    max_eps = .9\n",
    "    min_eps = .01\n",
    "    return min_eps + (max_eps-min_eps)*np.exp(-1. * i_episode / eps_decay)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is parametrized by some hyperparameters, defined in the cell below.\n",
    "\n",
    "* `stop_at` is the threshold for the average reward over the last `window` episodes. If the average exceeds `stop_at`, the training stops.\n",
    "* `window` is the window size for the running average plotted in the chart and used to threshold with `stop_at`\n",
    "* `batch_size` is the size of a mini-batch of experience used to update the parameters of the network `policy` during every step of the training\n",
    "* `gamma` is passed to the function `training_step`\n",
    "* `update_period` is the number of training episodes between copying the parameters from `policy` to `target`\n",
    "* `memory_size` is the number of the most recent tuples of experience stored\n",
    "* `n_episodes` is the maximum number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_at = 150\n",
    "window = 100\n",
    "batch_size = 128\n",
    "gamma = 1.0\n",
    "update_period = 5\n",
    "memory_size = 10000\n",
    "n_episodes = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the actual training code: each training episode starts with resetting the environment (`env.reset()`) and then it loops until the environment doesn't signal that the stop condition was reached. Every step of the internal loop consists of picking an action (either at random, or the best action according to the `policy` network), executing it (`env.step(action)`) and storing the new tuple of experience in `memory`. If there's at least `batch_size` worth of experience in the memory, a random subset of experience is selected using `np.random.choice` and `training_step` is called.\n",
    "\n",
    "During the event the total reward is summed up in the `episode_reward` variable, which is then stored in `total_rewards`. Every so often the chart is updated, the early stopping condition is verified and every `update_period` episodes the parameters are copied from `policy` to `target`.\n",
    "\n",
    "**Note:** This approach with storing experience for further use and selecting a random subset is a non-obvious one. The idea is that the learning examples given to a learning algorithm should be independent of each other, but experience from a single event is highly dependent. See what happens if you decrease the diversity by clearning the memory in the beginning of each episode, and what happens if you encourage more diversity by storing only some randomly selected tuples of experience in the memory and discarding the rest without using them even once for training.\n",
    "\n",
    "**Remark on performance:** Training takes time. Start it and leave it alone for some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "target.eval()\n",
    "\n",
    "for i_episode in range(n_episodes):    \n",
    "    state = env.reset()[0]\n",
    "    episode_reward = 0\n",
    "    while True:           \n",
    "        if np.random.random() <= p_exploration(i_episode):\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            policy.eval()\n",
    "            qvalues = policy(torch.tensor(state.reshape((1,-1)), dtype=torch.float))\n",
    "            action = int(qvalues.argmax().detach())\n",
    "        newstate, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward        \n",
    "        memory.append((state, action, reward, newstate, done))        \n",
    "        state = newstate\n",
    "                \n",
    "        if len(memory) >= batch_size:\n",
    "            indices = np.random.choice(len(memory), size=batch_size, replace=False)\n",
    "            training_step([memory[i] for i in indices], gamma=gamma)            \n",
    "        \n",
    "        if done:            \n",
    "            break   \n",
    "            \n",
    "    total_rewards.append(episode_reward)    \n",
    "    if (i_episode+1) % 100 == 0:\n",
    "        plot_chart(total_rewards, window)\n",
    "        \n",
    "    running_avg = np.mean(total_rewards[-window:])\n",
    "    if running_avg >= stop_at:\n",
    "        print(\"Ending at\", running_avg)\n",
    "        break\n",
    "                \n",
    "    if i_episode % update_period == 0:\n",
    "        target.load_state_dict(policy.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the agent is trained! The following cell will use the agent to display a full run of the environment. Observe how much longer this agent is capable of balancing the cart than the random agent from the beginning of the notebook. If you were luck the agent will go for 500 steps, at which point the environment stops, probably assuming that the agent can do this more or less indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy.eval()\n",
    "state = env.reset()[0]\n",
    "n = 0\n",
    "while True:\n",
    "    env.render()    \n",
    "    qvalues = policy(torch.tensor(state.reshape((1,-1)), dtype=torch.float))\n",
    "    action = int(qvalues.argmax().detach())    \n",
    "    state, _, done, _, _ = env.step(action)\n",
    "    n += 1\n",
    "    if done:\n",
    "        print(n)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
